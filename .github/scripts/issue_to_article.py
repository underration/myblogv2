#!/usr/bin/env python3
"""
Issue to Article Converter

Issueã®æœ¬æ–‡ã‹ã‚‰Markdownè¨˜äº‹ã‚’ç”Ÿæˆã—ã€ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«å‚ç…§ã«ç½®æ›ã™ã‚‹ã€‚
"""

import os
import re
import hashlib
import urllib.request
import urllib.error
from pathlib import Path
from typing import Optional

# ========== è¨­å®š ==========
ARTICLES_DIR = Path("src/content/articles")
IMAGES_DIR = Path("public/images/articles")

# è¨±å¯ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ƒï¼‰
ALLOWED_DOMAINS = [
    "github.com",
    "user-attachments",
    "githubusercontent.com",
    "avatars.githubusercontent.com",
    "raw.githubusercontent.com",
    "substackcdn.com",
    "substack-post-media.s3.amazonaws.com",
]

# ã‚µã‚¤ã‚ºåˆ¶é™
MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10MB per image
MAX_TOTAL_SIZE = 50 * 1024 * 1024  # 50MB total

# Content-Type to extension mapping
CONTENT_TYPE_TO_EXT = {
    "image/jpeg": ".jpg",
    "image/png": ".png",
    "image/gif": ".gif",
    "image/webp": ".webp",
    "image/svg+xml": ".svg",
}


def slugify(text: str, max_length: int = 50) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ãƒ©ãƒƒã‚°ã«å¤‰æ›"""
    # å°æ–‡å­—åŒ–
    slug = text.lower()
    # è‹±æ•°å­—ã¨ã‚¹ãƒšãƒ¼ã‚¹ä»¥å¤–ã‚’å‰Šé™¤
    slug = re.sub(r'[^a-z0-9\s-]', '', slug)
    # ã‚¹ãƒšãƒ¼ã‚¹ã‚’ãƒã‚¤ãƒ•ãƒ³ã«
    slug = re.sub(r'[\s_]+', '-', slug)
    # é€£ç¶šãƒã‚¤ãƒ•ãƒ³ã‚’1ã¤ã«
    slug = re.sub(r'-+', '-', slug)
    # å‰å¾Œã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤
    slug = slug.strip('-')
    # é•·ã•åˆ¶é™
    if len(slug) > max_length:
        slug = slug[:max_length].rstrip('-')
    return slug or "untitled"


def extract_frontmatter(body: str) -> tuple[dict, str]:
    """front matterã‚’æŠ½å‡º"""
    frontmatter = {}
    content = body

    # front matterã®æ¤œå‡ºï¼ˆ---ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ï¼‰
    fm_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', body, re.DOTALL)
    if fm_match:
        fm_text = fm_match.group(1)
        content = body[fm_match.end():]

        # è¡Œå˜ä½ã§ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼: å€¤ å½¢å¼ã®ã¿å¯¾å¿œï¼‰
        for line in fm_text.split('\n'):
            line = line.strip()
            if ':' in line:
                key, _, value = line.partition(':')
                key = key.strip()
                value = value.strip()
                # å¼•ç”¨ç¬¦ã‚’é™¤å»
                if (value.startswith('"') and value.endswith('"')) or \
                   (value.startswith("'") and value.endswith("'")):
                    value = value[1:-1]
                frontmatter[key] = value

    return frontmatter, content


def extract_image_urls(frontmatter: dict, content: str) -> list[tuple[str, str]]:
    """ç”»åƒURLã‚’æŠ½å‡ºã€‚(url, type) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚typeã¯ 'cover' ã¾ãŸã¯ 'body'"""
    urls = []
    seen = set()

    # front matterã‹ã‚‰coverç”»åƒã‚’æŠ½å‡º
    if 'cover' in frontmatter:
        cover_url = frontmatter['cover']
        if cover_url.startswith('http'):
            urls.append((cover_url, 'cover'))
            seen.add(cover_url)

    # æœ¬æ–‡ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º
    img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
    for match in re.finditer(img_pattern, content):
        url = match.group(2).strip()
        if url.startswith('http') and url not in seen:
            urls.append((url, 'body'))
            seen.add(url)

    return urls


def is_allowed_domain(url: str) -> bool:
    """URLãŒè¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯"""
    for domain in ALLOWED_DOMAINS:
        if domain in url:
            return True
    return False


def download_image(url: str, save_path: Path, total_downloaded: int) -> tuple[bool, int]:
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚æˆåŠŸæ™‚ã¯(True, bytes)ã€å¤±æ•—æ™‚ã¯(False, 0)ã‚’è¿”ã™"""
    if not url.startswith('https://'):
        print(f"  âš ï¸ HTTPSã§ã¯ãªã„URLã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return False, 0

    if not is_allowed_domain(url):
        print(f"  âš ï¸ è¨±å¯ã•ã‚Œã¦ã„ãªã„ãƒ‰ãƒ¡ã‚¤ãƒ³: {url}")
        return False, 0

    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=30) as response:
            content_type = response.headers.get('Content-Type', '').split(';')[0].strip()

            # Content-Typeãƒã‚§ãƒƒã‚¯
            if not content_type.startswith('image/'):
                print(f"  âš ï¸ ç”»åƒã§ã¯ãªã„Content-Type: {content_type}")
                return False, 0

            # æ‹¡å¼µå­ã‚’æ±ºå®š
            ext = CONTENT_TYPE_TO_EXT.get(content_type)
            if not ext:
                # URLã‹ã‚‰æ‹¡å¼µå­ã‚’æ¨æ¸¬
                url_path = url.split('?')[0]
                for e in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:
                    if url_path.lower().endswith(e):
                        ext = '.jpg' if e == '.jpeg' else e
                        break
                else:
                    ext = '.jpg'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            # æœ€çµ‚çš„ãªä¿å­˜ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ã‚’è¿½åŠ ï¼‰
            final_path = save_path.with_suffix(ext)

            # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            content_length = response.headers.get('Content-Length')
            if content_length and int(content_length) > MAX_IMAGE_SIZE:
                print(f"  âš ï¸ ç”»åƒã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {int(content_length) / 1024 / 1024:.1f}MB")
                return False, 0

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            data = response.read()
            if len(data) > MAX_IMAGE_SIZE:
                print(f"  âš ï¸ ç”»åƒã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {len(data) / 1024 / 1024:.1f}MB")
                return False, 0

            if total_downloaded + len(data) > MAX_TOTAL_SIZE:
                print(f"  âš ï¸ åˆè¨ˆã‚µã‚¤ã‚ºä¸Šé™ã«é”ã—ã¾ã—ãŸ")
                return False, 0

            # ä¿å­˜
            final_path.parent.mkdir(parents=True, exist_ok=True)
            final_path.write_bytes(data)
            print(f"  âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {final_path.name} ({len(data) / 1024:.1f}KB)")

            return True, len(data)

    except urllib.error.URLError as e:
        print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {url} - {e}")
        return False, 0
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {url} - {e}")
        return False, 0


def get_image_filename(url: str, img_type: str) -> str:
    """ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆæ‹¡å¼µå­ãªã—ï¼‰"""
    if img_type == 'cover':
        return 'cover'
    else:
        # URLã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        url_hash = hashlib.sha1(url.encode()).hexdigest()[:8]
        return f'img-{url_hash}'


def find_actual_image_path(base_path: Path) -> Optional[Path]:
    """æ‹¡å¼µå­ãªã—ã®ãƒ‘ã‚¹ã‹ã‚‰ã€å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™"""
    for ext in ['.jpg', '.png', '.gif', '.webp', '.svg']:
        path = base_path.with_suffix(ext)
        if path.exists():
            return path
    return None


def main():
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å…¥åŠ›ã‚’å–å¾—
    issue_number = os.environ.get('ISSUE_NUMBER', '0')
    issue_title = os.environ.get('ISSUE_TITLE', 'Untitled')
    issue_body = os.environ.get('ISSUE_BODY', '')

    print(f"ğŸ“ Issue #{issue_number}: {issue_title}")
    print("=" * 50)

    if not issue_body:
        print("âŒ Issueæœ¬æ–‡ãŒç©ºã§ã™")
        return

    # 1. front matteræŠ½å‡º
    frontmatter, content = extract_frontmatter(issue_body)
    print(f"ğŸ“‹ Front matter keys: {list(frontmatter.keys())}")

    # 2. slugæ±ºå®š
    slug = frontmatter.get('slug', '')
    if not slug:
        slug = slugify(issue_title)
    # å±é™ºæ–‡å­—ã‚’é™¤å»
    slug = re.sub(r'[./\\]', '', slug)
    print(f"ğŸ“Œ Slug: {slug}")

    # 3. ç”»åƒURLåé›†
    image_urls = extract_image_urls(frontmatter, content)
    print(f"ğŸ–¼ï¸ ç”»åƒURLæ•°: {len(image_urls)}")

    # 4. ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & URLâ†’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
    url_to_local: dict[str, str] = {}
    images_dir = IMAGES_DIR / slug
    total_downloaded = 0

    for url, img_type in image_urls:
        filename = get_image_filename(url, img_type)
        save_path = images_dir / filename  # æ‹¡å¼µå­ãªã—

        success, size = download_image(url, save_path, total_downloaded)
        if success:
            total_downloaded += size
            # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¢ã™
            actual_path = find_actual_image_path(save_path)
            if actual_path:
                # å…¬é–‹ãƒ‘ã‚¹ï¼ˆ/images/articles/slug/filename.extï¼‰
                local_path = f"/images/articles/{slug}/{actual_path.name}"
                url_to_local[url] = local_path

    print(f"ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰åˆè¨ˆ: {total_downloaded / 1024:.1f}KB")

    # 5. front matterã®cover URLã‚’ç½®æ›
    updated_frontmatter = frontmatter.copy()
    if 'cover' in frontmatter and frontmatter['cover'] in url_to_local:
        # Astroã®image()ç”¨ã«ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã«å¤‰æ›
        # /images/articles/slug/cover.jpg -> ../../../public/images/articles/slug/cover.jpg
        local_path = url_to_local[frontmatter['cover']]
        # ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼ã«å¤‰æ›ï¼ˆsrc/content/articles ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
        relative_path = f"../../../public{local_path}"
        updated_frontmatter['cover'] = relative_path

    # 6. æœ¬æ–‡ã®ç”»åƒURLã‚’ç½®æ›
    updated_content = content
    for url, local_path in url_to_local.items():
        # Markdownç”»åƒè¨˜æ³•ã®ç½®æ›
        # ![alt](url) -> ![alt](local_path)
        pattern = re.escape(url)
        updated_content = re.sub(
            rf'(!\[[^\]]*\]\(){pattern}(\))',
            rf'\g<1>{local_path}\g<2>',
            updated_content
        )

    # 7. è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    article_path = ARTICLES_DIR / f"{slug}.md"

    # front matterã‚’å†æ§‹ç¯‰
    fm_lines = ['---']
    for key, value in updated_frontmatter.items():
        # å€¤ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚„ç‰¹æ®Šæ–‡å­—ãŒå«ã¾ã‚Œã‚‹å ´åˆã®å‡¦ç†
        if isinstance(value, str) and (':' in value or '\n' in value or value.startswith(' ')):
            # å¼•ç”¨ç¬¦ã§å›²ã‚€å¿…è¦ãŒã‚ã‚‹å ´åˆ
            if '"' in value:
                value = f"'{value}'"
            else:
                value = f'"{value}"'
        fm_lines.append(f'{key}: {value}')
    fm_lines.append('---')
    fm_lines.append('')

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    article_content = '\n'.join(fm_lines) + updated_content
    article_path.parent.mkdir(parents=True, exist_ok=True)
    article_path.write_text(article_content, encoding='utf-8')

    print(f"âœ… è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {article_path}")
    print("=" * 50)


if __name__ == '__main__':
    main()

